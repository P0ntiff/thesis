



\documentclass[main]{subfiles}

\begin{document}


\chapter{Discussion}




%"While we have made a case for model agnosticism, this approach is not without its challenges. For example, getting a global understanding of the model may be hard if the model is very complex, due to the trade-off between flexibility and interpretability. To make matters worse, local explanations may be inconsistent with one another, since a flexible model may use a certain feature in different ways depending on the other features. In Ribeiro et al. (2016) we explained text models by selecting a small number of representative and non-redundant individual prediction explanations obtained via submodular optimization, similar in spirit to showing prototypes (Kim et al., 2014). However, it is unclear on how to extend this approach to domains such as images or tabular data, where the data itself is not sparse. In some domains, exact explanations may be required (e.g. for legal or ethical reasons), and using a black-box may be unacceptable (or even illegal). Interpretable models may also be more desirable when interpretability is much more important than accuracy, or when interpretable models trained on a small number of carefully engineered features are as accurate as black-box models."  Model-Agnostic Interpretability of Machine Learning

% mention low performance on oterh model archtiectures from Lit Review (ancona)


% Rethinking the inception architecture for computer vision

% As Adebayo et al. note, quantifying human visual perception is still an active area of research.

% Dataset observerations: center-looking models (center bias), and animals emphasis
KL-Lime discussion on role of itnerpretable models


Discuss developed software novelty

\end{document}