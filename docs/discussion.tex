



\documentclass[main]{subfiles}

\begin{document}


\chapter{Discussion}

\section{Strengths \& Limitations of Saliency Metrics}

There are a plethora of considerations around the design of proxy metrics for explanation quality. Some of these considerations are obvious, and have been mentioned by others:
\begin{itemize}
\item Human perception of explanation quality is difficult to quantify and is an active area of research (\ref{adebay}).
\item 
\end{itemize}


Other considerations are novel and were realised over the course of this project:

% Rethinking the inception architecture for computer vision

% As Adebayo et al. note, quantifying human visual perception is still an active area of research.



% Dataset observerations: center-looking models (center bias), and animals emphasis
%KL-Lime discussion on role of itnerpretable models


%Explanation consistency, performance, 

\section{Obstacles}

There was some disappointment around the bugs in SHAP's implementation that meant it couldn't be applied as easily to more complicated model architectures like . Other practitioners and researchers have noted similar difficulties in the GitHub issue list for the software package \cite{shaprepo}.


\section{Software Framework}


% pluggable support for attribution methods


% evaluation framework


\end{document}