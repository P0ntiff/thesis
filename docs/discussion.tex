



\documentclass[main]{subfiles}

\begin{document}


\chapter{Discussion}

\section{Strengths \& Limitations of Metric-based Evaluation}

There are a plethora of considerations around the design of proxy metrics for explanation quality. Some of these observations, ranging from obvious to more subtle, have been made in the prior art and were empirically confirmed:
\begin{itemize}
\item Human perception of explanation quality is difficult to quantify and is an active area of research (\ref{adebay}).
\item Errors of the method are not easy to distinguish from errors of the model \textbf{(cite)}.

\end{itemize}


Other novel conclusions were made however:

Method evaluation should account for localisation vs discriminative abilities of the method - a single localisation metric and a single pixel-wise metric are not . A more refined saliency metric may be able to account for connected components, edges and pixel-wise attribution intensity.

A set of higher level criteria, including Adebayo's randomisation-based sanity checks, implementation invariance as well as lower level, functionally-grounded saliency metrics, would be the fairest way to compensate for the subjectivity  of a single measure of explanation quality.

In a context where the underlying model predictions are constant for a panel of methods, as in this project, this can be somewhat controlled for.

% Rethinking the inception architecture for computer vision

% As Adebayo et al. note, quantifying human visual perception is still an active area of research.



% Dataset observerations: center-looking models (center bias), and animals emphasis
%KL-Lime discussion on role of itnerpretable models


%Explanation consistency, performance, 


\section{Comparing Approaches}
Backpropagation techniques require custom backward functions that override activation functions and gradient operators. This gives them implementation complexities for models with many \textit{types} of layer: this was previously mentioned as an explanation for DeepLIFT's poor performance on InceptionV3 and ResNet50 (Section \ref{saliency_insights}).

For GradCAM on the other hand, targeting only the final convolutional layer gives it a strength over others: it does not depend on complexity in earlier layers. Its limitation is that it can only be implemented for convolutional neural networks, therefore solely dominant in image-based tasks or other applications of that architecture. Other less implementation-dependent methods in the gradient class include Gradient * Input and SmoothGrad.

Perturbation techniques, including LIME but also those proposed by Fong \& Vedaldi \cite{perturb_fong} (`mask generation') and Zeiler \& Fergus (2014) \cite{zeilerfergus2013} ('Occlusion') have a very clear interpretation in what they iteratively reveal. However, the requirement to test a large number of small, progressively increasing occlusion patches was shown to suffer bad performance even on ``simpler'' architectures like VGG16. For modern, more complex architectures, perhaps those trained on higher resolution images or taking longer to compute a single prediction, the performance may make the method completely unviable, unless a very small number of explanations are required. The strength of LIME and the perturbation approach generally is its `off the shelf' ease of adoption, which is true for practitioners and researchers working on \textit{any} classification task.

\section{Obstacles \& Methodology Limitations}

There was some disappointment around the bugs in SHAP's implementation that meant it couldn't be applied to more complicated model architectures than VGG16. Other practitioners and researchers have noted similar difficulties in its GitHub issue list \cite{shaprepo}.

There was also some regret around saliency metrics being limited to the intuition based on pixel-wise attribution. The implicit procedure in the project's methodology was to ``massage'' attribution method outputs into a discriminative common form. This may have compromised or misrepresented the methods where a divergence from the authors' intent occurred (i.e. super-pixels for LIME, heatmaps for GradCAM). It would have been fairer analysis to critique them on their own criteria (i.e. localisation of bounding boxes for GradCAM) \textit{as well as} the saliency metrics and qualitative criteria used in this project. This would also have formed a positive contribution towards reproducibility.

\section{Software Framework}

Existing software packages combine only gradient-based methods, or single methods. This extra explanation firepower can be largely redundant when some method outputs are so strikingly similar (e.g. Figure \ref{adebayoimg} in Related Work). A strong contribution of this project was to combine different approaches and offer a more diverse set of explanations for image data, to overall offer a better explanation for any one instance. For example, Figures \ref{panelimg} and \ref{panel2img} in the Methodology highlight very different input space features for even a single model's prediction.

Modular support for attribution methods and evaluation metrics was achieved through the object-oriented approach to the testing framework (Figure \ref{flow_image} in the Methodology).
% pluggable support for attribution methods


% evaluation framework


\end{document}