
\documentclass[main]{subfiles}

\begin{document}

\chapter{Introduction}

\section{Background} \label{sec:background}

``State of the art'' machine learning models now regularly achieve above expert-level performance in fields as diverse as medical imaging and language translation. However, their lack of interpretability prevents their adoption in many of the fields that would benefit the most from them. These domains are often ones where the decisions made have a tangible impact on people's lives. Decision-makers in those domains are unlikely to use the predictions made by a highly accurate but opaque model because of their requirement for trust and accountability.

The requirement for explainability is not in order to replace human experts, but to understand contradictions between expert and algorithm. For example, a radiologist might disagree with a diagnosis made by a model trained to predict pneumonia from chest X-rays, and attributing the error to a factor that one or the other relied upon would be helpful. Was the model relying on some unrelated part of the scan (a spurious feature) or was the radiologist failing to pick up on a subtle pneumonia differential? The former was observed in practice after applying an explanation technique to a convolutional neural network (CNN) trained on X-ray data \cite{xray}. 

In the domains that require trust, explanations for model predictions are as important as low rates of incorrect predictions. This is the counter-argument to the notion that a highly accurate model need not be interpretable to be effective: the premise of effectiveness requires a level of trust that a model relies on unbiased data and non-spurious features, two guarantees that are not at all provided by an objective function that seeks only to minimise prediction error. With poor visibility into the factors that a model relies upon, machine learning researchers tend to use model performance metrics as the basis for arguing a new model architecture is superior. This disconnect between performance in the sense of test set accuracy and performance in the sense of accountability (lack of bias or spurious features) and reliability (sensical behaviour) is not ideal.

Interpretability in a Facebook algorithm recommending product categories, for example, might not be seen as important as interpretability in a cancer diagnosis model, though the possibility of unethical model behaviour from reliance on biased data is as tangible in both domains. In one study of 200 sentiment analysis classifiers, several were found to have significant race and gender bias \cite{bias}. The consequences of errors can certainly be higher in some domains however - a poor Netflix recommendation is not as disastrous as a naive algorithm used in government decision-making, such as the ``robo-debt'' scheme recently employed by the Australian Government \cite{nous}.

\subsection*{Approaches to Interpretability}

There is fortunately an active literature aimed at addressing this `black-box' critique in machine learning. The top-level distinction among approaches is to either use inherently intepretable models to achieve explainability, or take complex, black-box models and find techniques to isolate and explain a piece of their complexity, such as an individual prediction.

The first approach includes model families with low complexity like linear/logistic regression, decision trees, k-nearest neighbours and Naive Bayes. Within these families are both parametric and non-parametric techniques, which suggests that lack of interpretability is more related to model complexity than a particular type of formulation. This empirically observed trade-off between accuracy and interpretability is discussed further in the next section\footnote{There is a view by some researchers (\cite{rudin}) that in many domains the accuracy vs interpretability trade-off does not exist, and thus there is a responsibility to use equally effective, inherently interpretable models for high-stakes decisions where those are available.}. Since many of these models are often too simplistic for obtaining competitive performance, the motivation to attack the `black-box' critique from this angle is quite low. Instead, methods to introduce interpretability to modern, high-performance models are a more studied and popular approach to take in the literature, as in the second approach.

The second approach includes ``feature attribution'' or ``feature importance'' methods, which compute a weight score for each feature in the input space to measure its contribution to an output class. For example, a CNN classifier predicting ``tree'' would be expected to rely heavily on green pixels of leaves. This class includes model-specific techniques for neural network architectures, like those based on activations in a hidden layer, and model-agnostic techniques that are compatible with any model family. Within both sub-classes are a variety of techniques, with varying levels of model agnosticity and task compatibility. For example, some methods are designed solely for CNNs and are therefore mainly suited for image-related tasks. 

Importantly there are both global feature attribution methods, which calculate each feature's contribution to a model at large, as well as local methods, which attempt to explain a single prediction. This project has focused on the latter. Local methods are dominant in the literature for modern architectures - when the dimensionality of the data is high, such as in visual data, or when the number of parameters is too high to make conclusions about global model behaviour (as in most modern architectures) this tends to be the only effective approach to interpretability. An author of one local, model-agnostic method notes that understanding these models globally ``[..] would require understanding of the underlying prediction function at all locations of the input space" \cite{local}. 

\subsection*{Accuracy vs Interpretability}

As deep learning and other state-of-the-art model families proliferate in their typical number of parameters, global behaviour has become even less explainable. Researchers maintain some intuitions about the impact of architectural design decisions, though not on predictive behaviour. For example, filters within a CNN model have been shown to act as `object detectors' of patterns, shapes and other connected regions \cite{objectdetect}, though these per-layer intuitions don't explain how a network of dozens of layers will determine a husky from a wolf (a recent approach in the literature, however, has looked at abstracting model behaviour into `concept' vector encodings to capture model behaviour across  filters and layers (Net2Vec \cite{net2vec}, TCAV \cite{tcav})).

Feature attribution methods can therefore re-introduce transparency into complex, non-linear models and highlight predictive biases in the context of individual predictions. They can also reveal unexpected features involved in a prediction, such as the spurious features mentioned in the previous X-ray data example, or bugs that could lead to exploitation of adversarial examples \cite{adversary}. Note these methods do not seek to add causal interpretability to the models they are applied to, only to isolate and highlight a piece of complexity in a way that might make sense to a person reviewing the explanation. This does not make them shallow - the benefit of the `post-training' approach is that model designers have more flexibility in their choice of models and fewer restrictive assumptions about model complexity need to be made\footnote{The counter-argument made by those who argue for the inherently interpretable model approach is that there is no guarantee these explanations are faithful to the model, and that they extend the authority of the black box instead of making it a ``glass box'' \cite{rudin}.}. More model-agnostic methods, with the least restrictive requirements, are not well understood in context with model-specific ones in terms of this accuracy and interpretability trade-off.

\subsection*{Existing Evaluations}

Comparisons of feature attribution methods do exist, though typically either in a qualitative context, as a pairwise comparison, or within a single class of methods. They are not normally compared on practical considerations like performance or adoptability in terms of task and model compatibility. They are however compared haphazardly on explanation quality, which is a difficult criteria to design. Lack of evaluation is partly due to differences in method formulation and output representation, but also this difficulty in finding objective proxy metrics of explanation quality.

% Finding desirable criteria for explanation techniques to have, as well as tool development for generating explanations, are two active research areas.


%\section{Summary of Motivation}

%Research motivation:
%\begin{itemize}
%\item Provide a stronger understanding of the context of different feature attribution approaches
%\item Support the creation of more adoptable models
%\item Help diagnose spurious features and 
%\end{itemize}
%"Independent verification of published claims for the purpose of credibility confirmation, extension
%and building a ‘body of knowledge’ is a standard scientific practice" (Reproducibility in Machine Learning-Based Studies: An Example of Text Mining)

%The variety of approaches in the literature - as well as the number of works providing technique iteration and relatively few unification or evaluation studies in comparison - mirrors the same experimental and leaderboard-driven motivations of model proposals supposing a new "state of the art". In this sense the motivation for more interpretability is the same as the motivation for more reproducibility, and therefore easier tools for interpretability are one step on the road to re-aligning research incentives away from performance solely, instead towards accountability, reliability, reproducibility and other equally valid criteria for a "state-of-the-art" model.

%Application motivation:

\section{Project Overview}
This project has sought to evaluate a panel of feature attribution methods representative of different approaches to the interpretability problem. The aim was to highlight their relative strengths and weaknesses and thereby increase the understanding of the benefits of one method's approach over another.

Two other  key contributions made over the course of the project have been a quantitative evaluation framework for explanation quality in the image classification context, and the development of a software package to collect image data explanations for multiple underlying methods at scale.

In summary the project aims have been to:
\begin{enumerate}
	\item \textbf{Examine and evaluate} a panel of feature attribution methods using proxy metrics of explanation quality supported by analysis on performance, use cases and other criteria.

	\item \textbf{Develop} an attribution software package for testing methods at scale, with modular support for different metrics, methods and models, making it easier for researchers to collect explanations and build more adoptable models.
	
\end{enumerate}


\newpage

\subsection*{Project Scope} \label{sec:intro_scope}

Section \ref{sec:background} introduced a broad motivation for interpretability though this project has focused specifically on image classification for two main reasons.

Firstly, many interpretability techniques from before the deep learning era have been studied in this domain, and many deep learning methods continue to be developed and tested in this domain on modern CNN architectures. The natural visual aspect of image data explanations has also made computer vision a dominant venue for interpretability research, with important applications such as medical imaging.
 
Secondly, well-annotated datasets and pre-trained, `off the shelf' models are more easily acquired in this domain. This availability allowed for richer evaluation metrics and the removal of model training as a project requirement.

A more detailed scope is provided at the beginning of the Methodology section, including a description of the specific datasets, models, and feature attribution methods used.


\subsection*{Report Overview}

Chapter 2 overviews the available feature attribution methods and existing approaches to method evaluation. Chapter 3 breaks down the project's methodology in terms of particular milestones, including the software and evaluation metrics that were designed. Chapter 4 lists evaluation results from quantitative and qualitative standpoints. Chapter 5 provides a discussion on the project's methodology and the limitations encountered, along with recommendations for future work, and finally Chapter 6 provides a conclusion on the project's contribution.



\end{document}